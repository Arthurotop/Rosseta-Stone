{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61f08c1",
   "metadata": {},
   "source": [
    "Ici on va pré traiter les données pour les faire rentrer dans un modele (GRU OU LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fea87c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5675f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\idirs\\Desktop\\M2IA\\projet_rosette\\processing_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e7bd480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137860 lines for fr txt and 137860 lines for en txt\n"
     ]
    }
   ],
   "source": [
    "# Charger en listes de phrases alignées\n",
    "with open(\"../data/raw_data/small_vocab_fr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    fr_sentences = f.read().strip().split(\"\\n\")\n",
    "\n",
    "with open(\"../data/raw_data/small_vocab_en.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    en_sentences = f.read().strip().split(\"\\n\")\n",
    "\n",
    "print(len(fr_sentences), \"lines for fr txt and\", len(en_sentences), \"lines for en txt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ee38a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('new jersey is sometimes quiet during autumn , and it is snowy in april .', \"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\")\n"
     ]
    }
   ],
   "source": [
    "# Aligne en paires (anglais, français)\n",
    "\n",
    "pairs = list(zip(en_sentences, fr_sentences))\n",
    "print(pairs[0])  # (en, fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e06396",
   "metadata": {},
   "source": [
    "Nettoyer les phrases\n",
    "\n",
    "* Minuscule\n",
    "\n",
    "* Retirer la ponctuation ou la garder (au choix, si dataset petit je conseille de la garder mais séparée en tokens).\n",
    "\n",
    "* Normaliser espaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976bfced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence: str):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "pairs = [(clean_sentence(en), clean_sentence(fr)) for en, fr in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14574816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation\n",
    "\n",
    "en_tokens = [en.split() for en, fr in pairs]\n",
    "fr_tokens = [fr.split() for en, fr in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0641a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire vocabulaires (word → id, id → word)\n",
    "def build_vocab(token_lists, min_freq=1):\n",
    "    counter = Counter([tok for sent in token_lists for tok in sent])\n",
    "    vocab = {word for word, freq in counter.items() if freq >= min_freq}\n",
    "    vocab = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + sorted(list(vocab))\n",
    "    word2id = {w:i for i,w in enumerate(vocab)}\n",
    "    id2word = {i:w for i,w in enumerate(vocab)}\n",
    "    return word2id, id2word\n",
    "\n",
    "en_word2id, en_id2word = build_vocab(en_tokens)\n",
    "fr_word2id, fr_id2word = build_vocab(fr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "268849c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([128, 98, 93, 163, 146, 57, 19, 4, 9, 95, 93, 162, 91, 14, 5], [1, 218, 163, 117, 238, 61, 241, 166, 40, 4, 119, 154, 117, 217, 109, 45, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "# Convertir phrases → séquences d’IDs\n",
    "\n",
    "def encode_sentence(tokens, word2id, add_sos=False, add_eos=False):\n",
    "    ids = []\n",
    "    if add_sos:\n",
    "        ids.append(word2id[\"<sos>\"])\n",
    "    for t in tokens:\n",
    "        ids.append(word2id.get(t, word2id[\"<unk>\"]))\n",
    "    if add_eos:\n",
    "        ids.append(word2id[\"<eos>\"])\n",
    "    return ids\n",
    "\n",
    "pairs_ids = []\n",
    "for en, fr in zip(en_tokens, fr_tokens):\n",
    "    src_ids = encode_sentence(en, en_word2id)\n",
    "    tgt_ids = encode_sentence(fr, fr_word2id, add_sos=True, add_eos=True)\n",
    "    pairs_ids.append((src_ids, tgt_ids))\n",
    "\n",
    "print(pairs_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba07fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/preprocessed_data/pairs_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pairs_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "794be951",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/preprocessed_data/pairs_ids.pkl\", \"rb\") as f:\n",
    "    pairs_ids_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b70f8",
   "metadata": {},
   "source": [
    "Split Train / Val / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b50fbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(pairs_ids)\n",
    "k = 0.7 # percentage data pour la partie training\n",
    "q = (1 - k)/2  # percentage data pour la partie test/val\n",
    "n = len(pairs_ids)\n",
    "train_pairs = pairs_ids[:int(k*n)]\n",
    "val_pairs   = pairs_ids[int(k*n):int((k+q)*n)]\n",
    "test_pairs  = pairs_ids[int(((k+q)*n)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f844547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96502/96502 [00:00<00:00, 746396.21it/s]\n",
      "100%|██████████| 20679/20679 [00:00<00:00, 816673.69it/s]\n",
      "100%|██████████| 20679/20679 [00:00<00:00, 738570.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 352 mots uniques (98.88%)\n",
      "Val:   336 mots uniques (94.38%)\n",
      "Test:  334 mots uniques (93.82%)\n",
      "\n",
      "Doublons train: 9176\n",
      "Doublons val:   512\n",
      "Doublons test:  444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def count_unique_words(pairs):\n",
    "    \"\"\"\n",
    "    Compte le nombre de mots uniques dans les phrases cibles d'un ensemble de paires.\n",
    "\n",
    "    Args:\n",
    "        pairs (list): liste de tuples (src, tgt)\n",
    "\n",
    "    Returns:\n",
    "        unique_words_count (int): nombre de mots uniques\n",
    "        duplicates (int): nombre de phrases doublons\n",
    "        unique_words_set (set): ensemble des mots uniques rencontrés\n",
    "    \"\"\"\n",
    "    seen_phrases = set()\n",
    "    unique_words = set()\n",
    "    duplicates = 0\n",
    "\n",
    "    for _, tgt in tqdm(pairs):\n",
    "        phrase_tuple = tuple(tgt)  # pour rendre la liste hashable\n",
    "        if phrase_tuple not in seen_phrases:\n",
    "            seen_phrases.add(phrase_tuple)\n",
    "            unique_words.update(tgt)\n",
    "        else:\n",
    "            duplicates += 1\n",
    "\n",
    "    return len(unique_words), duplicates, unique_words\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Comptage\n",
    "# ==============================\n",
    "word_train, dtr, used_train = count_unique_words(train_pairs)\n",
    "word_val, dva, used_val = count_unique_words(val_pairs)\n",
    "word_test, dte, used_test = count_unique_words(test_pairs)\n",
    "\n",
    "total_word = len(fr_id2word)\n",
    "\n",
    "# ==============================\n",
    "# Affichage\n",
    "# ==============================\n",
    "print(f\"Train: {word_train} mots uniques ({100 * word_train / total_word:.2f}%)\")\n",
    "print(f\"Val:   {word_val} mots uniques ({100 * word_val / total_word:.2f}%)\")\n",
    "print(f\"Test:  {word_test} mots uniques ({100 * word_test / total_word:.2f}%)\\n\")\n",
    "\n",
    "print(f\"Doublons train: {dtr}\")\n",
    "print(f\"Doublons val:   {dva}\")\n",
    "print(f\"Doublons test:  {dte}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9067851",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"train\": train_pairs,\n",
    "    \"val\": val_pairs,\n",
    "    \"test\": test_pairs,\n",
    "    \"en_word2id\": en_word2id,\n",
    "    \"fr_word2id\": fr_word2id,\n",
    "    \"en_id2word\": en_id2word,\n",
    "    \"fr_id2word\": fr_id2word\n",
    "}, \"data/preprocessed_data/processed_data.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rosette",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
